{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fadd276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intelligibility eval\n"
     ]
    }
   ],
   "source": [
    "print('intelligibility eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c9c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (4.46.3)\n",
      "Requirement already satisfied: filelock in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from transformers) (4.46.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: torch in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.9.86)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: jiwer in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (4.0.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from jiwer) (3.9.7)\n",
      "Collecting pandas==1.1.5\n",
      "  Downloading pandas-1.1.5-cp38-cp38-manylinux1_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from pandas==1.1.5) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from pandas==1.1.5) (2025.2)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from pandas==1.1.5) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas==1.1.5) (1.16.0)\n",
      "Downloading pandas-1.1.5-cp38-cp38-manylinux1_x86_64.whl (9.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.0.3\n",
      "    Uninstalling pandas-2.0.3:\n",
      "      Successfully uninstalled pandas-2.0.3\n",
      "Successfully installed pandas-1.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install jiwer\n",
    "!pip install pandas==1.1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65ddd67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472cdffa5c994128a70b3f49661d26fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='preprocessor_config.json', max=357.0, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a86247e9fe0243bc854e245cd44d861e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='tokenizer_config.json', max=1.0, style=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd49fa39c3154874a85066ff573d5039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='vocab.json', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429f5ac8cc794ff6a000d71855e5355f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='tokenizer.json', max=1.0, style=Progres…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b62ea36c8864f04985ef0fa429e2443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='merges.txt', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bbf540c12aa464b9783d3ed1d6b85c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='normalizer.json', max=1.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e25073500924240aee353535d0f73b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='added_tokens.json', max=1.0, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "310aa9b037ba4102b0eabcc1773e507d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='special_tokens_map.json', max=1.0, styl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages/transformers/pipelines/automatic_speech_recognition.py:312: FutureWarning: `max_new_tokens` is deprecated and will be removed in version 4.49 of Transformers. To remove this warning, pass `max_new_tokens` as a key inside `generate_kwargs` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "#model_id = \"nilc-nlp/distil-whisper-coraa-mupe-asr\"\n",
    "model_id = \"sidleal/distil-whisper-tarsila-asr-v1-200k\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    #model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    "    model_id, torch_dtype=torch_dtype, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    torch_dtype=torch_dtype,\n",
    "    return_timestamps=True,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "def trancript_with_distil(file_path):\n",
    "    result = pipe(file_path)\n",
    "    return result[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "610a4195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sidleal/anaconda3/envs/fs2/lib/python3.8/site-packages/transformers/models/whisper/generation_whisper.py:509: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      " outra televisão dera aquar\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "result = trancript_with_distil(\"/home/sidleal/sources/FastSpeech2/output/result/cmltts_entoa_prossegue/600_outra televisão de araraquara.wav\")\n",
    "print(\"----------------------------------------\")\n",
    "print(result)\n",
    "print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7929b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jiwer\n",
    "import jiwer.transforms as tr\n",
    "\n",
    "cer_transform = tr.Compose(\n",
    "    [\n",
    "        jiwer.ToLowerCase(),\n",
    "        jiwer.RemoveMultipleSpaces(),\n",
    "        jiwer.Strip(),\n",
    "        jiwer.ReduceToListOfListOfChars(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# It's the jiwer default transform\n",
    "wer_transform = jiwer.Compose([\n",
    "    jiwer.ToLowerCase(),\n",
    "    jiwer.RemoveMultipleSpaces(),\n",
    "    jiwer.Strip(),\n",
    "    jiwer.ReduceToListOfListOfWords(),\n",
    "])\n",
    "\n",
    "def compute_cer(reference, hypothesis):\n",
    "    reference = reference.lower()\n",
    "    hypothesis = hypothesis.lower()\n",
    "    cer = jiwer.wer(reference, hypothesis, truth_transform=cer_transform, hypothesis_transform=cer_transform)\n",
    "    return cer\n",
    "\n",
    "def compute_wer(reference, hypothesis):\n",
    "    reference = reference.lower()\n",
    "    hypothesis = hypothesis.lower()\n",
    "    wer = jiwer.wer(reference, hypothesis, truth_transform=wer_transform, hypothesis_transform=wer_transform)\n",
    "    return wer\n",
    "\n",
    "\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZÇÃÀÁÂÊÉÍÓÔÕÚÛabcdefghijklmnopqrstuvwxyzçãàáâêéíóôõũúû1234567890%\\\\-\\n/\\\\ \"\n",
    "\n",
    "\n",
    "def replace_special_tokens_and_normalize(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    map_words = {\n",
    "        \"éh\": \"eh\",\n",
    "        \"ehm\": \"eh\",\n",
    "        \"ehn\": \"eh\",\n",
    "        \"hum\": \"uh\",\n",
    "        \"hm\": \"uh\",\n",
    "        \"uhm\": \"uh\",\n",
    "        \"hã\": \"ah\",\n",
    "        \"ãh\": \"ah\",\n",
    "        \"ã\":  \"ah\",\n",
    "        \"hmm\": \"uh\",\n",
    "        \"mm\": \"uh\",\n",
    "        \"mhm\": \"uh\"\n",
    "    }\n",
    "\n",
    "    text = re.sub(\"h+\", \"h\", text)\n",
    "    text = re.sub(\"[^{}]\".format(alphabet+\" \"), \" \", text)\n",
    "    text = re.sub(\"[ ]+\", \" \", text)\n",
    "\n",
    "    words = text.split(' ')\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word == '' or word == ' ':\n",
    "            continue\n",
    "        if word in map_words:\n",
    "            new_words.append(map_words[word])\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "\n",
    "def calculate_wer_cer(reference, hypothesis):\n",
    "    if reference.strip() == '' or hypothesis.strip() == '':\n",
    "        return 1, 1\n",
    "    wer = compute_wer(reference, hypothesis)\n",
    "    cer = compute_cer(reference, hypothesis)\n",
    "    return wer, cer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aac5f639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 0: bom um bom artista é o que desempenha o papel na peça de acordo com o que ele está está fazendo eu acho que o que ele está o papel que ele está desempenhando ah que eles eh a gente perceba que que realmente ele está trabalhando bem sei lá não sei tem tantos bons artistas\n",
      "[0] Running: cd /home/sidleal/sources/FastSpeech2 && python synthesize.py --text 'bom um bom artista é o que desempenha o papel na peça de acordo com o que ele está está fazendo eu acho que o que ele está o papel que ele está desempenhando ah que eles eh a gente perceba que que realmente ele está trabalhando bem sei lá não sei tem tantos bons artistas' --speaker_id 13 --restore_step 600000 --mode single -p config/cmltts_entoa_prossegue/preprocess.yaml -m config/cmltts_entoa_prossegue/model.yaml -t config/cmltts_entoa_prossegue/train.yaml\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'cd /home/sidleal/sources/FastSpeech2 && python synthesize.py --text 'bom um bom artista é o que desempenha o papel na peça de acordo com o que ele está está fazendo eu acho que o que ele está o papel que ele está desempenhando ah que eles eh a gente perceba que que realmente ele está trabalhando bem sei lá não sei tem tantos bons artistas' --speaker_id 13 --restore_step 600000 --mode single -p config/cmltts_entoa_prossegue/preprocess.yaml -m config/cmltts_entoa_prossegue/model.yaml -t config/cmltts_entoa_prossegue/train.yaml' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m command_to_execute \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcd /home/sidleal/sources/FastSpeech2 && python synthesize.py --text \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhuman_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m --speaker_id 13 --restore_step 600000 --mode single -p config/cmltts_entoa_prossegue/preprocess.yaml -m config/cmltts_entoa_prossegue/model.yaml -t config/cmltts_entoa_prossegue/train.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Running: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcommand_to_execute\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommand_to_execute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# We use shell=True because the command uses shell features (like passing arguments)\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/fs2/lib/python3.8/subprocess.py:516\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[0;32m--> 516\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    517\u001b[0m                                  output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process\u001b[38;5;241m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'cd /home/sidleal/sources/FastSpeech2 && python synthesize.py --text 'bom um bom artista é o que desempenha o papel na peça de acordo com o que ele está está fazendo eu acho que o que ele está o papel que ele está desempenhando ah que eles eh a gente perceba que que realmente ele está trabalhando bem sei lá não sei tem tantos bons artistas' --speaker_id 13 --restore_step 600000 --mode single -p config/cmltts_entoa_prossegue/preprocess.yaml -m config/cmltts_entoa_prossegue/model.yaml -t config/cmltts_entoa_prossegue/train.yaml' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "#sintetizar\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "\n",
    "file_path = \"/home/sidleal/sources/entoa-tts/experiments/sp_234_l1_transc_cer_wer.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    print(f\"Row {index}: {row['human']}\")\n",
    "    human_value = row['human']\n",
    "    command_to_execute = f\"cd /home/sidleal/sources/FastSpeech2 && python synthesize.py --text '{human_value}' --speaker_id 13 --restore_step 600000 --mode single -p config/cmltts_entoa_prossegue/preprocess.yaml -m config/cmltts_entoa_prossegue/model.yaml -t config/cmltts_entoa_prossegue/train.yaml\"\n",
    "\n",
    "    print(f\"[{index}] Running: {command_to_execute}\")\n",
    "\n",
    "    result = subprocess.run(\n",
    "        command_to_execute,\n",
    "        shell=True, # We use shell=True because the command uses shell features (like passing arguments)\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        check=True\n",
    "    )\n",
    "    print(result.stdout.strip())\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05288eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "file_name = 'prossegue_cer_wer_output.csv'\n",
    "\n",
    "folder_path = '/home/sidleal/sources/FastSpeech2/output/result/cmltts_entoa_prossegue/sp_234'\n",
    "\n",
    "with open(file_name, 'a', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    csv_data = [\"audio_name\", \"human_text\", \"human_text_norm\", \"transcript_original\", \"transcript_synthesized\", \"cer_original\", \"cer_synthesized\", \"wer_original\", \"wer_synthesized\"]\n",
    "    writer.writerow(csv_data)\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            audio_name = filename[:-8]\n",
    "            original_audio_name = filename[:-4]\n",
    "            synth_audio_name = filename[:-4] + \"_synthesized.wav\"\n",
    "\n",
    "            with open(f\"{folder_path}/{filename}\", encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                text_norm = replace_special_tokens_and_normalize(text)\n",
    "\n",
    "                transcript_ori = trancript_with_distil(f\"{folder_path}/{original_audio_name}\")\n",
    "                transcript_syn = trancript_with_distil(f\"{folder_path}/{synth_audio_name}\")\n",
    "\n",
    "                transcript_ori_norm = replace_special_tokens_and_normalize(transcript_ori)\n",
    "                transcript_syn_norm = replace_special_tokens_and_normalize(transcript_syn)\n",
    "\n",
    "                ori_wer_cer = calculate_wer_cer(text_norm, transcript_ori_norm)\n",
    "                syn_wer_cer = calculate_wer_cer(text_norm, transcript_syn_norm)\n",
    "\n",
    "                print(\"--------------------------------\")\n",
    "                print(audio_name)\n",
    "                print(\"-\", text)\n",
    "                print(\"-\", text_norm)\n",
    "                print(\"-\", original_audio_name)\n",
    "                print(\"-\", synth_audio_name)\n",
    "                print(\"-\", transcript_ori_norm)\n",
    "                print(\"-\", transcript_syn_norm)\n",
    "                print(ori_wer_cer)\n",
    "                print(syn_wer_cer)\n",
    "\n",
    "                csv_data = [audio_name, text, text_norm, transcript_ori_norm, transcript_syn_norm, ori_wer_cer[1], syn_wer_cer[1], ori_wer_cer[0], syn_wer_cer[0]]\n",
    "                writer.writerow(csv_data)\n",
    "\n",
    "            #break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4d8923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER ORIGINAL: 13.70\n",
      "WER SINTETIZADO: 23.86\n",
      "T-statistic: -6.1082 (-6.108212784547283)\n",
      "P-value: 0.0000 (5.242308079569724e-09)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "t_statistic, p_value = stats.ttest_ind(df[\"wer_original\"], df[\"wer_synthesized\"])\n",
    "\n",
    "print(f\"WER ORIGINAL: {df[\"wer_original\"].mean()*100:.2f}\")\n",
    "print(f\"WER SINTETIZADO: {df[\"wer_synthesized\"].mean()*100:.2f}\")\n",
    "\n",
    "print(f\"T-statistic: {t_statistic:.4f} ({t_statistic})\")\n",
    "print(f\"P-value: {p_value:.4f} ({p_value})\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ba50a",
   "metadata": {},
   "source": [
    "# f5tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0658bc29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio_name</th>\n",
       "      <th>human_text</th>\n",
       "      <th>human_text_norm</th>\n",
       "      <th>transcript_original</th>\n",
       "      <th>transcript_synthesized</th>\n",
       "      <th>cer_original</th>\n",
       "      <th>cer_synthesized</th>\n",
       "      <th>wer_original</th>\n",
       "      <th>wer_synthesized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sr16k_mono_0027-CP011_1250.69-1261.3</td>\n",
       "      <td>Ô Jorge, é o seguinte, a gente esqueceu de avi...</td>\n",
       "      <td>ô jorge é o seguinte a gente esqueceu de avisa...</td>\n",
       "      <td>o jorge é o seguinte a gente esqueceu de avisa...</td>\n",
       "      <td>eu já é o seguinte a gente esqueceu de avisar ...</td>\n",
       "      <td>0.037267</td>\n",
       "      <td>0.211180</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.387097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sr16k_mono_0038-CP015_950.037-979.634</td>\n",
       "      <td>E eu gostaria de lembrar que um dos motivos da...</td>\n",
       "      <td>e eu gostaria de lembrar que um dos motivos da...</td>\n",
       "      <td>eu gostaria de lembrar que um dos motivos dess...</td>\n",
       "      <td>eu gostaria de lembrar que os motivos da da re...</td>\n",
       "      <td>0.104623</td>\n",
       "      <td>0.085158</td>\n",
       "      <td>0.184211</td>\n",
       "      <td>0.118421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sr16k_mono_0067-CP546_1576.56-1591.05</td>\n",
       "      <td>É. é que Veja bem, essa experiência, eu gostar...</td>\n",
       "      <td>é é que veja bem essa experiência eu gostaria ...</td>\n",
       "      <td>é eh que veja bem essa experiência eu gostaria...</td>\n",
       "      <td>é que veja bem essa experiência eu gostaria qu...</td>\n",
       "      <td>0.102041</td>\n",
       "      <td>0.073469</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.212766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sr16k_mono_0082-CP707_1508.09-1518.64</td>\n",
       "      <td>Mas eu gostaria... Claudio, dá dá só uma uma u...</td>\n",
       "      <td>mas eu gostaria claudio dá dá só uma uma uma a...</td>\n",
       "      <td>mas eu gostaria caldo dá dá só uma uma aperfei...</td>\n",
       "      <td>você gostaria de é de dada só uma uma aperfeiç...</td>\n",
       "      <td>0.112583</td>\n",
       "      <td>0.172185</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.321429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sr16k_mono_0079-CP623_853.479-861.621</td>\n",
       "      <td>Foi nesse contraponto que eu tentei compreende...</td>\n",
       "      <td>foi nesse contraponto que eu tentei compreende...</td>\n",
       "      <td>então foi nesse contraponto que eu tentei comp...</td>\n",
       "      <td>foi nesse contrário conto que eu tentei compre...</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.421053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              audio_name  \\\n",
       "0   sr16k_mono_0027-CP011_1250.69-1261.3   \n",
       "1  sr16k_mono_0038-CP015_950.037-979.634   \n",
       "2  sr16k_mono_0067-CP546_1576.56-1591.05   \n",
       "3  sr16k_mono_0082-CP707_1508.09-1518.64   \n",
       "4  sr16k_mono_0079-CP623_853.479-861.621   \n",
       "\n",
       "                                          human_text  \\\n",
       "0  Ô Jorge, é o seguinte, a gente esqueceu de avi...   \n",
       "1  E eu gostaria de lembrar que um dos motivos da...   \n",
       "2  É. é que Veja bem, essa experiência, eu gostar...   \n",
       "3  Mas eu gostaria... Claudio, dá dá só uma uma u...   \n",
       "4  Foi nesse contraponto que eu tentei compreende...   \n",
       "\n",
       "                                     human_text_norm  \\\n",
       "0  ô jorge é o seguinte a gente esqueceu de avisa...   \n",
       "1  e eu gostaria de lembrar que um dos motivos da...   \n",
       "2  é é que veja bem essa experiência eu gostaria ...   \n",
       "3  mas eu gostaria claudio dá dá só uma uma uma a...   \n",
       "4  foi nesse contraponto que eu tentei compreende...   \n",
       "\n",
       "                                 transcript_original  \\\n",
       "0  o jorge é o seguinte a gente esqueceu de avisa...   \n",
       "1  eu gostaria de lembrar que um dos motivos dess...   \n",
       "2  é eh que veja bem essa experiência eu gostaria...   \n",
       "3  mas eu gostaria caldo dá dá só uma uma aperfei...   \n",
       "4  então foi nesse contraponto que eu tentei comp...   \n",
       "\n",
       "                              transcript_synthesized  cer_original  \\\n",
       "0  eu já é o seguinte a gente esqueceu de avisar ...      0.037267   \n",
       "1  eu gostaria de lembrar que os motivos da da re...      0.104623   \n",
       "2  é que veja bem essa experiência eu gostaria qu...      0.102041   \n",
       "3  você gostaria de é de dada só uma uma aperfeiç...      0.112583   \n",
       "4  foi nesse contrário conto que eu tentei compre...      0.148148   \n",
       "\n",
       "   cer_synthesized  wer_original  wer_synthesized  \n",
       "0         0.211180      0.161290         0.387097  \n",
       "1         0.085158      0.184211         0.118421  \n",
       "2         0.073469      0.170213         0.212766  \n",
       "3         0.172185      0.214286         0.321429  \n",
       "4         0.194444      0.263158         0.421053  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_name = 'cer_wer_output.csv'\n",
    "\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de81885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('cer_wer_output.csv')\n",
    "\n",
    "file_name = 'cer_wer_output_v2.csv'\n",
    "\n",
    "folder_path = 'resulting_audios_cv-fn'\n",
    "\n",
    "with open(file_name, 'a', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    csv_data = [\"audio_name\", \"human_text\", \"human_text_norm\", \"transcript_original\", \"transcript_synthesized_yourtts\", \"transcript_synthesized_f5tts_cv\", \"cer_original\", \"cer_synthesized_yourtts\", \"cer_synthesized_f5tts_cv\", \"wer_original\", \"wer_synthesized_yourtts\", \"wer_synthesized_f5tts_cv\"]\n",
    "    writer.writerow(csv_data)\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".wav\"):\n",
    "            audio_name = filename[:-4]\n",
    "            print(audio_name)\n",
    "\n",
    "            audio_key = f\"sr16k_mono_{audio_name[6:]}\"\n",
    "            print(audio_key)\n",
    "\n",
    "            row = df.query(f\"audio_name == '{audio_key}'\")\n",
    "            print(row['audio_name'].iloc[0])\n",
    "            print(row['human_text_norm'].iloc[0])\n",
    "            print(\"-------\")\n",
    "            \n",
    "            text_norm = replace_special_tokens_and_normalize(row['human_text_norm'].iloc[0])\n",
    "\n",
    "            transcript_f5cv = trancript_with_distil(f\"{folder_path}/{filename}\")\n",
    "            transcript_f5cv_norm = replace_special_tokens_and_normalize(transcript_f5cv)\n",
    "\n",
    "            f5cv_wer_cer = calculate_wer_cer(text_norm, transcript_f5cv_norm)\n",
    "\n",
    "            print(\"-\", text_norm)\n",
    "            print(\"-\", transcript_f5cv_norm)\n",
    "            print(f5cv_wer_cer)\n",
    "\n",
    "            csv_data = [audio_key, row[\"human_text\"].iloc[0], row[\"human_text_norm\"].iloc[0], row[\"transcript_original\"].iloc[0], row[\"transcript_synthesized\"].iloc[0], transcript_f5cv_norm, row[\"cer_original\"].iloc[0], row[\"cer_synthesized\"].iloc[0], f5cv_wer_cer[1], row[\"wer_original\"].iloc[0], row[\"wer_synthesized\"].iloc[0], f5cv_wer_cer[0]]\n",
    "            writer.writerow(csv_data)\n",
    "\n",
    "            #break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c430e840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER ORIGINAL: 13.60\n",
      "WER SINTETIZADO F5 CV: 18.12\n",
      "T-statistic: -3.0251 (-3.025088762473149)\n",
      "P-value: 0.0028 (0.0028187078083642975)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import pandas as pd\n",
    "\n",
    "file_name = 'cer_wer_output_v2.csv'\n",
    "\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "t_statistic, p_value = stats.ttest_ind(df[\"wer_original\"], df[\"wer_synthesized_f5tts_cv\"])\n",
    "\n",
    "print(f\"WER ORIGINAL: {df[\"wer_original\"].mean()*100:.2f}\")\n",
    "print(f\"WER SINTETIZADO F5 CV: {df[\"wer_synthesized_f5tts_cv\"].mean()*100:.2f}\")\n",
    "\n",
    "print(f\"T-statistic: {t_statistic:.4f} ({t_statistic})\")\n",
    "print(f\"P-value: {p_value:.4f} ({p_value})\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8431b903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER ORIGINAL: 23.77\n",
      "WER SINTETIZADO F5 CV: 18.12\n",
      "T-statistic: 3.6120 (3.6120189458795777)\n",
      "P-value: 0.0004 (0.0003859933928752929)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import pandas as pd\n",
    "\n",
    "file_name = 'cer_wer_output_v2.csv'\n",
    "\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "t_statistic, p_value = stats.ttest_ind(df[\"wer_synthesized_yourtts\"], df[\"wer_synthesized_f5tts_cv\"])\n",
    "\n",
    "print(f\"WER ORIGINAL: {df[\"wer_synthesized_yourtts\"].mean()*100:.2f}\")\n",
    "print(f\"WER SINTETIZADO F5 CV: {df[\"wer_synthesized_f5tts_cv\"].mean()*100:.2f}\")\n",
    "\n",
    "print(f\"T-statistic: {t_statistic:.4f} ({t_statistic})\")\n",
    "print(f\"P-value: {p_value:.4f} ({p_value})\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fs2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
